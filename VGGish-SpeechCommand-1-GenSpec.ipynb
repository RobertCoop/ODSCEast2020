{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# VGGish - Speech Commands - Generate Spectrograms\n\n**Pipeline Step 1: Audio Preprocessing**\n\nThis is the first step in the audio classification pipeline. It processes all ~105,000 audio files from the Speech Commands dataset and converts each one into a log-mel spectrogram suitable for the VGGish network.\n\n**Pipeline overview:**\n1. **Generate spectrograms** (this notebook) -- Convert WAV files to 96x64 spectrograms\n2. Generate embeddings -- Pass spectrograms through VGGish to produce 128-dim vectors\n3. Train classifier -- Use embeddings or spectrograms to classify spoken words\n\n**What this notebook does:**\n- Walks the Speech Commands dataset directory to catalog all WAV files with their labels\n- Converts each audio file to a VGGish-format spectrogram (96 time frames x 64 mel bands)\n- Flags invalid entries (files that don't produce exactly one 96x64 spectrogram)\n- Saves the spectrogram array and metadata to disk for use in subsequent notebooks"
  },
  {
   "cell_type": "markdown",
   "source": "## Setup\n\nImport the VGGish library and its spectrogram generation utilities. The VGGish pipeline expects audio at 16 kHz and produces spectrograms with a window hop of 0.96 seconds, meaning each ~1-second audio clip yields a single 96x64 spectrogram frame.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/odsc/vggish/lib/models/research/audioset/vggish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vggish_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.96, 0.96)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vggish_params.EXAMPLE_HOP_SECONDS, vggish_params.EXAMPLE_WINDOW_SECONDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/odsc/vggish/env/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import six\n",
    "import soundfile\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Building the Dataset Catalog\n\nWalk the Speech Commands directory tree to build a DataFrame of all WAV files. Each subdirectory name is the spoken word label (e.g., \"zero\", \"yes\", \"stop\"). The dataset contains 105,835 files across 35 word categories plus a `_background_noise_` directory (6 files) which is included here but later filtered out.\n\nThe class distribution is imbalanced: the 20 \"core\" words have ~3,700-4,000 samples each, while the 15 \"auxiliary\" words have ~1,400-2,100 samples each.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import vggish_input\n",
    "import vggish_postprocess\n",
    "import vggish_slim\n",
    "\n",
    "pca_params = '/home/ubuntu/odsc/vggish/lib/vggish_pca_params.npz'\n",
    "ckpt = '/home/ubuntu/odsc/vggish/lib/vggish_model.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_files = {\n",
    "    'file_name' : [],\n",
    "    'label': []\n",
    "}\n",
    "\n",
    "rootDir = '/home/ubuntu/audio/speech_commands'\n",
    "for dirName, subdirList, fileList in os.walk(rootDir):\n",
    "    for fname in fileList:\n",
    "        if fname.endswith('.wav'):\n",
    "            wav_files['label'].append(os.path.basename(dirName))\n",
    "            wav_files['file_name'].append(os.path.join(dirName, fname))\n",
    "\n",
    "df = pd.DataFrame(data=wav_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/ubuntu/audio/speech_commands/zero/8a90cf...</td>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/ubuntu/audio/speech_commands/zero/173ae7...</td>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/ubuntu/audio/speech_commands/zero/eb76bc...</td>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/ubuntu/audio/speech_commands/zero/978240...</td>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/ubuntu/audio/speech_commands/zero/246328...</td>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name label\n",
       "0  /home/ubuntu/audio/speech_commands/zero/8a90cf...  zero\n",
       "1  /home/ubuntu/audio/speech_commands/zero/173ae7...  zero\n",
       "2  /home/ubuntu/audio/speech_commands/zero/eb76bc...  zero\n",
       "3  /home/ubuntu/audio/speech_commands/zero/978240...  zero\n",
       "4  /home/ubuntu/audio/speech_commands/zero/246328...  zero"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Verifying Single-File Spectrogram Generation\n\nBefore processing the full dataset, we verify that a single WAV file produces the expected 96x64 spectrogram. The `wavfile_to_examples()` function returns a batch of spectrograms -- for 1-second clips, this should be exactly one spectrogram with shape `(1, 96, 64)`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105835, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "five                  4052\n",
       "zero                  4052\n",
       "yes                   4044\n",
       "seven                 3998\n",
       "no                    3941\n",
       "nine                  3934\n",
       "down                  3917\n",
       "one                   3890\n",
       "go                    3880\n",
       "two                   3880\n",
       "stop                  3872\n",
       "six                   3860\n",
       "on                    3845\n",
       "left                  3801\n",
       "eight                 3787\n",
       "right                 3778\n",
       "off                   3745\n",
       "four                  3728\n",
       "three                 3727\n",
       "up                    3723\n",
       "dog                   2128\n",
       "wow                   2123\n",
       "house                 2113\n",
       "marvin                2100\n",
       "bird                  2064\n",
       "happy                 2054\n",
       "cat                   2031\n",
       "sheila                2022\n",
       "bed                   2014\n",
       "tree                  1759\n",
       "backward              1664\n",
       "visual                1592\n",
       "follow                1579\n",
       "learn                 1575\n",
       "forward               1557\n",
       "_background_noise_       6\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Batch Spectrogram Generation\n\nProcess all 105,835 audio files. For each file:\n1. Generate the VGGish spectrogram using `wavfile_to_examples()`\n2. If the result has exactly 1 frame (shape `(1, 96, 64)`), store it and mark as valid\n3. If the result has 0 or >1 frames (due to audio being too short or too long), mark as invalid\n\nSome files produce invalid spectrograms -- typically the `_background_noise_` files which are longer than 1 second. These will be filtered out before training.\n\nThe spectrograms are stored in a pre-allocated NumPy array of shape `(105835, 96, 64)`. Progress is printed every 1,000 files.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/audio/speech_commands/zero/8a90cf67_nohash_0.wav'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = df['file_name'][0]\n",
    "fname"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Saving Results\n\nSave the DataFrame (with file paths, labels, and validity flags) to CSV and the spectrogram array to a binary file. These outputs are used by the subsequent notebooks:\n- `wavfile_df.csv` -- metadata for all 105,835 audio files\n- `wavfile_spec.dat` -- raw binary dump of the spectrogram NumPy array (105,835 x 96 x 64 float64 values)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = vggish_input.wavfile_to_examples(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 96, 64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data = np.empty((df.shape[0], 96, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "36000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "61000\n",
      "63000\n",
      "65000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n"
     ]
    }
   ],
   "source": [
    "for ind, row in df.iterrows():\n",
    "    data = vggish_input.wavfile_to_examples(row.file_name)\n",
    "    if data.shape[0] != 1:\n",
    "        df.loc[ind, 'valid'] = False\n",
    "        continue\n",
    "    audio_data[ind, :, :] = data\n",
    "    df.loc[ind, 'valid'] = True\n",
    "    if ind % 1000==0:\n",
    "        print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('wavfile_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wavfile_spec.dat', 'wb') as f:\n",
    "    audio_data.tofile(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}